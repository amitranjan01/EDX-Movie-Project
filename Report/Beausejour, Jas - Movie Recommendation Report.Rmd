---
title: "Movie Ratings Project"
author: "by Jasmyn Beausejour"
output: 
  html_notebook: 
    toc: yes
---

# Introduction

In this project, we will be using all the tools and methods learned throughout the HarvardX Professional Certificate in Data Science program on edX to build a movie recommendation system.

We will first create the data sets with code that has been provided by the staff at HarvardX. Then, we will do exploratory analysis to get a better understanding of the data. We will then train a machine learning algorithm on the **edx set** to finally help us make recommendations on the **validation set**.

Our output will be 4 different files:

1. A report in PDF format, which we will obtain by publishing this R Notebook in HTML and printing as PDF

2. This report in RMD file

3. A simple R script that generates our recommendations (only the machine learning part of this report without the prose)

4. A *submission.csv* file that will contain the list of movies and our recommendations

# Create edx set, validation set, and submission file

First, we load a few libraries to ensure everything will run smoothly.

```{r}
library(tidyverse)
library(caret)
```

Then, we run the code provided by the staff, which creates an **edx** data frame and a **validation** data frame.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

# Learners will develop their algorithms on the edx set
# For grading, learners will run algorithm on validation set to generate ratings

validation <- validation %>% select(-rating)

# Ratings will go into the CSV submission file below:

write.csv(validation %>% select(userId, movieId) %>% mutate(rating = NA),
          "submission.csv", na = "", row.names=FALSE)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


# Exploratory data analysis

This section starts with the answers to the quizz online, and then I diverge into my own exploratory analysis.

### Quiz from edX

In our environment, we now have two objects. The **edx set** has `r nrow(edx)` observations by `r length(edx)` variables. The **validation set** has `r nrow(validation)` observations by `r length(validation)` variables. 

Unsurprisingly, the 5 variables included in the **validation set** are also present in the **edx set**, which also has a variable called **rating**, which is the purpose of this paper: predicting the rating for the validation set.

The distribution of ratings given in the **edx set** is as follows.

```{r, results="asis"}
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
library(knitr)

if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
library(kableExtra)

ratings_freqency <- edx %>% 
  group_by(rating) %>% 
  count() %>% 
  rename("Rating"=rating, "Frequency"=n) %>% 
  mutate("Of total"=paste(round(100*Frequency/nrow(edx),2),"%",sep=""))


kable(ratings_freqency, align = rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

The **edx set** is unique on every row, but the number of movies represented is not equal to the number of rows. To count the number of unique movies represented in the set, we use this quick piece of code. Note that we are using the **movieId** variable in case there might be some typos in the **title** variable.

```{r}
length(unique(edx$movieId))
```

We can make the same analysis for the number of unique users.

```{r}
length(unique(edx$userId))
```

From a genre perspective, we are asked to find out how many ratings there are for movies that fit the following genres: drama, comedy, thriller, and romance. Of course, a movie might fit different genre.

What we will do here is create a new data frame that will stipulate, for each row, which genre that movie fits. We will then count by genre.

```{r}
genre_analysis <- edx %>% mutate(Is.Drama = str_detect(genres, "Drama"),
                                 Is.Comedy = str_detect(genres,"Comedy"),
                                 Is.Thriller = str_detect(genres, "Thriller"),
                                 Is.Romance = str_detect(genres, "Romance"))

genre_analysis <- data_frame("Genre"=c("Drama", "Comedy", "Thriller", "Romance"),
                              "Ratings included"=c(sum(genre_analysis$Is.Drama),
                                                   sum(genre_analysis$Is.Comedy),
                                                   sum(genre_analysis$Is.Thriller), 
                                                   sum(genre_analysis$Is.Romance)))
kable(genre_analysis, align = rep("c",2)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Now, if we want to see which movie has the greatest number of ratings, we use this piece of code.

```{r}
movies_by_number_of_rankings <- edx %>% 
  group_by(title) %>% 
  count() %>% 
  ungroup() %>% 
  arrange(-n) %>% 
  rename("Movie"=title, "Ratings"=n)

kable(head(movies_by_number_of_rankings), align = rep("c",2)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

To see which ratings are given the most often, we can reorder a table we made above.

```{r}
ratings_freqency <- ratings_freqency %>% arrange(-Frequency)

kable(ratings_freqency, align = rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

We can also evaluate how frequent are the "half star ratings".

```{r}
half_vs_full_ratings <- edx %>% 
  mutate("Type"=ifelse(rating %in% c(1,2,3,4,5),"Full","Half")) %>% 
  group_by(Type) %>% 
  select(Type) %>% 
  count() %>% 
  rename("Ratings"=n) %>% 
  mutate("Of total"=paste(round(100*Ratings/nrow(edx),2),"%",sep=""))

kable(half_vs_full_ratings, align = rep("c",2)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

### Additional exploratory analysis based on course book

Drawing from section 71 of **Introduction to Data Science: Data Analysis and Prediction Algorithms with R** by Rafael A. Irizarry, which is available at https://rafalab.github.io/dsbook/, we make a few observations about the data. In fact, most of the code and ideas of this section draw heavily from this book.

First of all, we can see that each user has provided different ratings for different movies. In the table below, we show the ratings given by UserIds 13 through 25 for the5 movies with the most ratings. We can see that not evey user has ranked every movie.

```{r, echo=FALSE}
keep <- edx %>% 
  count(movieId) %>% 
  top_n(5, n) %>% 
  .$movieId

tab <- edx %>% 
  filter(movieId%in%keep) %>% 
  filter(userId %in% c(13:25)) %>% 
  select(userId, title, rating) %>% 
  mutate(title = str_remove(title, ", The"),
         title = str_remove(title, ":.*")) %>%
  spread(title, rating)

kable(tab) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

We can get a sense of the sparcity of the data by looking at a matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating. As we can see here, the data is very sparse.

```{r}
if(!require(rafalib)) install.packages("rafalib", repos = "http://cran.us.r-project.org")
library(rafalib)

users <- sample(unique(edx$userId), 100)
rafalib::mypar()
edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users") %>% 
  abline(h=0:100+0.5, v=0:100+0.5, col = "lightgrey")

```

Of course, some movies receive a lot more ratings than others. This is not surprising as blockbusters are expected to be rated more frequently than niche movies. We can get a sense of the distribution.

```{r}
edx %>% 
  count(movieId) %>%
  arrange(-n) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  labs(title="Movies by rating count", y="Movies", x="Number of ratings")
```

Of course, the same can be said about users: some are much more active than others.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
edx %>% 
  count(userId) %>%
  arrange(-n) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10()+
  labs(title="Users by rating count", y="Users", x="Number of ratings")
```

### Additional exploratory analysis based on personal view

Let's first identify whether different users tend to rate movies more harshly than others

```{r}
Avg_by_user <- edx %>% 
  group_by(userId) %>% 
  summarize(avg=mean(rating)) 
```

On average, users give an average score of `r mean(Avg_by_user$avg)`. That being said, there is indeed a distribution around this mean.

```{r}  
Avg_by_user %>% ggplot(aes(avg)) +
  geom_histogram(bins = 30, color = "black")+
  labs(title="Distribution of average score by users", x="Average score", y="Number of users")
```

Next, we would like to create a list of all the genres that are represented. Here, we notice that genres are using this style when there are more than 1: **Genre1|Genre2|Genre3**. The list of genres that are represented can be derived as follows.

```{r}
genres <- unique(c(str_split_fixed(edx$genres,pattern="\\|",n=Inf)))
genres <- genres[genres !=""&genres !="(no genres listed)"]
print(genres)
```

Now, it would be interesting to see the variation in average score by genre. We also highlight the standard deviation in ratings and the frequency of each genre. They are ordered from the highest average score to the lowest average score.

```{r}
Genres_Stats <- data_frame("Genre"=genres,
                           "Average Rating"="", 
                           "Standard Deviation"="",
                           "Ratings included"="")

for (i in 1:length(genres)) {
  current_genre=genres[i]
  Genres_Stats[i,2] <- edx %>% filter(str_detect(genres,current_genre))%>%
    summarize("Average"=round(mean(rating),2)) %>% .$Average
  Genres_Stats[i,3] <- edx %>% filter(str_detect(genres,current_genre))%>%
    summarize("StDev"=round(sd(rating),2)) %>% .$StDev
  Genres_Stats[i,4] <- edx %>% filter(str_detect(genres,current_genre))%>%
    count()
  }

Genres_Stats$`Average Rating` <- as.numeric(Genres_Stats$`Average Rating`)

Genres_Stats %>%
  arrange(-`Average Rating`) %>% 
  kable(align=rep("c",4)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)

```

# Training machine learning algorithm

### Creating the training and test set

For this portion of the paper, we will randomly select 20% of the **edx set** to serve as our test set.

```{r}
set.seed(755)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
```

To make sure we don’t include users and movies in the test set that do not appear in the training set, we remove these entries using the semi_join function:

```{r}
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```


### Mesure of success

Typically, we would evaluate our algorithm based on the Loss function, or residual mean squared error (RMSE). We define $y_{u,i}$ as the rating for movie $i$ by user $u$ and denote our prediction with $\hat{y}_{i,u}$. The RMSE is then defined as: 

$$
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
$$
with $N$ being the number of user/movie combinations and the sum occurring over all these combinations.

Let's write a function that computes the **RMSE** for vectors of ratings and their corresponding predictions:

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

Basically, for RMSE greater than 1, it means that on average we are off by more than one star. We therefore try to minimize RMSE.

We are also told that we will be evaluated based on accuracy, not RMSE. This means that we are tasked with making categorical predictions as opposed to numerical predictions. The only values we should be predicting are therefore **0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, or 5**.

The final measure of our success will be calculated as follows:

$$
\mbox{Accuracy}=\frac{Correct\ Predictions}{Total\ Predictions}
$$
Let's write a function that computes the **accuracy** for vectors of ratings and their corresponding predictions:

```{r}
accuracy <- function(true_ratings, predicted_ratings){
  mean(true_ratings==predicted_ratings)
}
```

Let us test the functions:

```{r}
true_ratings <- c(1,2,4.5,4,5,0.5)
predicted_ratings <- c(1,2.5,4.5,4,2,1)

RMSE(true_ratings,predicted_ratings)
accuracy(true_ratings,predicted_ratings)

rm(true_ratings,predicted_ratings)
```

### Simple linear model based on user bias and movie bias

One of the downsides of this approach will be that we will have to round our predicted ratings because we expect the linear model to return non-round predictions.

Let us simply check what our success metrics would be if we were to predict the average for each observation.

```{r}
average_rating <- mean(train_set$rating)

naiveRMSE <- RMSE(test_set$rating, average_rating)
naiveAccuracy <- accuracy(test_set$rating, average_rating)
```

As we go along, we will be comparing different approaches. Let's start by creating a results table with this naive approach:

```{r}
rmse_results <- data_frame(Method = "Just the average", RMSE = naiveRMSE, Accuracy =naiveAccuracy)

kable(rmse_results,align=rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```
Of course, we obtain an accuracy of 0 since the average is not a round number. If we try that we get:

```{r}
rounded_average_rating <- round(average_rating/2)*2

roundednaiveRMSE <- RMSE(test_set$rating,rounded_average_rating)
roundednaiveAccuracy <- accuracy(test_set$rating,rounded_average_rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "Just the average (rounded)",
                                     RMSE=roundednaiveRMSE,
                                     Accuracy = roundednaiveAccuracy))
kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```
We can see here that we have increased our accuracy to about `r paste(round(100*rmse_results[2,3],2),"%",sep="")`, but our RMSE has increased, which is not surprising.

We can now factor in the fact that certain movies tend to get much better ratings than others. We call this the **movie_effect**.

```{r}
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(movie_effect = mean(rating - average_rating))
```

We can see that these estimates vary substantially:

```{r}
movie_avgs %>% qplot(movie_effect, geom ="histogram", bins = 10, data = ., color = I("black"))
```
  
We remember that our average rating is bout 3.5, so a 1.5 movie_effect means a perfect score of 5. 
  
We can now try to estimate each observation of the test set using this movie_effect. To be clear, this is the model we are trying, where $$\mu$$ is the average rating, $$b_i$$ is the movie_effect and $$\varepsilon_{u,i}$$ is the error term:
  
  $$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

  
```{r}
predicted_ratings <- average_rating + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$movie_effect

movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating)
movie_effect_Accuracy <- accuracy(test_set$rating,predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Movie Effect",
                                     RMSE=movie_effect_RMSE,
                                     Accuracy = movie_effect_Accuracy))

rounded_movie_effect_RMSE <- RMSE(round(predicted_ratings/2)*2,test_set$rating)
rounded_movie_effect_Accuracy <- accuracy(round(predicted_ratings/2)*2,test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Movie Effect (rounded)",
                                     RMSE=rounded_movie_effect_RMSE,
                                     Accuracy = rounded_movie_effect_Accuracy))

kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)

```
  
As we can see, compared to the rounded average, we have improved our RMSE as well as our Accuracy.

Of course, we know that some users also rate movies differently based on their personal characteristics.

```{r}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")+
  labs(title="Count of users by average rating")
```

Here we can calculate the **user_effect** which is a user-specific effect once the **movie_effect** has been taken into consideration.

We can now try to estimate each observation of the test set using this movie_effect. To be clear, this is the model we are trying, where $$\mu$$ is the average rating, $$b_i$$ is the movie_effect, $$b_u$$ is the user-specific effect and $$\varepsilon_{u,i}$$ is the error term:
  
  $$
Y_{u,i} = \mu + b_i +b_u + \varepsilon_{u,i}
$$

```{r}
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(user_effect = mean(rating - average_rating - movie_effect))
```

Let's create the non-rounded and rounded predictors and see how it improves our model.

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = average_rating + movie_effect + user_effect) %>%
  .$pred

movieanduser_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 
movieanduser_effect_Accuracy <- accuracy(predicted_ratings, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Movie and User Effect",
                                     RMSE=movieanduser_effect_RMSE,
                                     Accuracy = movieanduser_effect_Accuracy))

rounded_movieanduser_effect_RMSE <- RMSE(round(predicted_ratings/2)*2, test_set$rating) 
rounded_movieanduser_effect_Accuracy <- accuracy(round(predicted_ratings/2)*2, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Movie and User Effect (rounded)",
                                     RMSE=rounded_movieanduser_effect_RMSE,
                                     Accuracy = rounded_movieanduser_effect_Accuracy))

kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

As we can see in this table, however, we only achieved 5% improvement in RMSE with our **movie_effect**, namely from `r naiveRMSE` to `r movie_effect_RMSE`. The next portion of this paper focuses on regularizing out estimates to make sure that movies that are rated very infrequently do not get substantially impacted by the **movie_effect** term in the model.

Let's look at the top 10 worst and best movies based on $\hat{b}_i$. First, let's create a database that connects `movieId` to movie title:

```{r}
movie_titles <- edx %>% 
  select(movieId, title) %>%
  distinct()
```

Here are the 10 best movies according to our estimate and how often they were rated:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(movie_effect)) %>% 
  select(title, movie_effect,n) %>%
  rename("Title"=title,"Movie Effect"=movie_effect,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

And here are the 10 worst:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(movie_effect) %>% 
  select(title, movie_effect,n) %>%
  rename("Title"=title,"Movie Effect"=movie_effect,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

This is probably not fair. We hare heavily benefiting or penalizing obscure movies when we actually have very little data points for them.

We will now estimate the **regularized_movie_effect** such that:

$$
Regularized\ Movie\ Effect = \hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$

where $n_i$ is the number of ratings made for movie $i$.

Let's compute these regularized estimates of **regularized_movie_effect** using 
$\lambda=3$. Later, we will see why we picked 3. 

```{r}
lambda <- 3

movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(reg_movie_effects = sum(rating - average_rating)/(n()+lambda), n_i = n()) 
```

To see how the estimates shrink, let's make a plot of the regularized estimates versus the least squares estimates.

```{r regularization-shrinkage}
data_frame("Original Movie Effect" = movie_avgs$movie_effect, 
           "Regularized Movie Effect" = movie_reg_avgs$reg_movie_effects, 
           n = movie_reg_avgs$n_i) %>%
    ggplot(aes(`Original Movie Effect`, `Regularized Movie Effect`, size=sqrt(n))) + 
        geom_point(shape=1, alpha=0.5)
```

Let's look at the top 10 best movies based on $\hat{b}_i(\lambda)$:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_reg_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(reg_movie_effects)) %>% 
  select(title, reg_movie_effects,n) %>%
  rename("Title"=title,"Reg. Movie Effect"=reg_movie_effects,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

And now the 10 worst:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_reg_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(reg_movie_effects) %>% 
  select(title, reg_movie_effects,n) %>%
  rename("Title"=title,"Reg. Movie Effect"=reg_movie_effects,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Let's see if this improves our results further:

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = average_rating + reg_movie_effects) %>%
  .$pred

reg_movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 
reg_movie_effect_Accuracy <- accuracy(predicted_ratings, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Regularized Movie Effect",
                                     RMSE=reg_movie_effect_RMSE,
                                     Accuracy = reg_movie_effect_Accuracy))

round_reg_movie_effect_RMSE <- RMSE(round(predicted_ratings/2)*2, test_set$rating) 
round_reg_movie_effect_Accuracy <- accuracy(round(predicted_ratings/2)*2, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Regularized Movie Effect (rounded)",
                                     RMSE=round_reg_movie_effect_RMSE,
                                     Accuracy = round_reg_movie_effect_Accuracy))

kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)

```

