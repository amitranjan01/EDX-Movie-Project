---
title: "Movie Ratings Project"
author: "by Jasmyn Beausejour"
output: 
  html_notebook: 
    toc: yes
---

# Introduction

In this project, we will be using all the tools and methods learned throughout the HarvardX Professional Certificate in Data Science program on edX to build a movie recommendation system.

We will first create the data sets with code that has been provided by the staff at HarvardX. Then, we will do exploratory analysis to get a better understanding of the data. We will then train a machine learning algorithm on the **edx set** to finally help us make recommendations on the **validation set**.

Our output will be 4 different files:

1. A report in PDF format, which we will obtain by publishing this R Notebook in HTML and printing as PDF

2. This report in RMD file

3. A simple R script that generates our recommendations (only the machine learning part of this report without the prose)

4. A *submission.csv* file that will contain the list of movies and our recommendations

# Create edX set, Validation Set, and Submission File

First, we load a few libraries to ensure everything will run smoothly.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(caret)
```

Then, we run the code provided by the staff, which creates an **edx** data frame and a **validation** data frame.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

# Learners will develop their algorithms on the edx set
# For grading, learners will run algorithm on validation set to generate ratings

validation <- validation %>% select(-rating)

# Ratings will go into the CSV submission file below:

write.csv(validation %>% select(userId, movieId) %>% mutate(rating = NA),
          "submission.csv", na = "", row.names=FALSE)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


# Exploratory Data Analysis

This section starts with the answers to the quizz online, and then I diverge into my own exploratory analysis.

### Quiz from edX

In our environment, we now have two objects. The **edx set** has `r nrow(edx)` observations by `r length(edx)` variables. The **validation set** has `r nrow(validation)` observations by `r length(validation)` variables. 

Unsurprisingly, the 5 variables included in the **validation set** are also present in the **edx set**, which also has a variable called **rating**, which is the purpose of this paper: predicting the rating for the validation set.

The distribution of ratings given in the **edx set** is as follows.

```{r message=FALSE, warning=FALSE, paged.print=FALSE, results="asis"}
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
library(knitr)

if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
library(kableExtra)

ratings_freqency <- edx %>% 
  group_by(rating) %>% 
  count() %>% 
  rename("Rating"=rating, "Frequency"=n) %>% 
  mutate("Of total"=paste(round(100*Frequency/nrow(edx),2),"%",sep=""))


kable(ratings_freqency, align = rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

The **edx set** is unique on every row, but the number of movies represented is not equal to the number of rows. To count the number of unique movies represented in the set, we use this quick piece of code. Note that we are using the **movieId** variable in case there might be some typos in the **title** variable.

```{r}
length(unique(edx$movieId))
```

We can make the same analysis for the number of unique users.

```{r}
length(unique(edx$userId))
```

From a genre perspective, we are asked to find out how many ratings there are for movies that fit the following genres: drama, comedy, thriller, and romance. Of course, a movie might fit different genre.

What we will do here is create a new data frame that will stipulate, for each row, which genre that movie fits. We will then count by genre.

```{r}
genre_analysis <- edx %>% mutate(Is.Drama = str_detect(genres, "Drama"),
                                 Is.Comedy = str_detect(genres,"Comedy"),
                                 Is.Thriller = str_detect(genres, "Thriller"),
                                 Is.Romance = str_detect(genres, "Romance"))

genre_analysis <- data_frame("Genre"=c("Drama", "Comedy", "Thriller", "Romance"),
                              "Ratings included"=c(sum(genre_analysis$Is.Drama),
                                                   sum(genre_analysis$Is.Comedy),
                                                   sum(genre_analysis$Is.Thriller), 
                                                   sum(genre_analysis$Is.Romance)))
kable(genre_analysis, align = rep("c",2)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Now, if we want to see which movie has the greatest number of ratings, we use this piece of code.

```{r}
movies_by_number_of_rankings <- edx %>% 
  group_by(title) %>% 
  count() %>% 
  ungroup() %>% 
  arrange(-n) %>% 
  rename("Movie"=title, "Ratings"=n)

kable(head(movies_by_number_of_rankings), align = rep("c",2)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

To see which ratings are given the most often, we can reorder a table we made above.

```{r}
ratings_freqency <- ratings_freqency %>% arrange(-Frequency)

kable(ratings_freqency, align = rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

We can also evaluate how frequent are the "half star ratings".

```{r}
half_vs_full_ratings <- edx %>% 
  mutate("Type"=ifelse(rating %in% c(1,2,3,4,5),"Full","Half")) %>% 
  group_by(Type) %>% 
  select(Type) %>% 
  count() %>% 
  rename("Ratings"=n) %>% 
  mutate("Of total"=paste(round(100*Ratings/nrow(edx),2),"%",sep=""))

kable(half_vs_full_ratings, align = rep("c",2)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

### Additional Exploratory Analysis Based on Course Book

Drawing from section 71 of **Introduction to Data Science: Data Analysis and Prediction Algorithms with R** by Rafael A. Irizarry, which is available at https://rafalab.github.io/dsbook/, we make a few observations about the data. In fact, most of the code and ideas of this section draw heavily from this book.

First of all, we can see that each user has provided different ratings for different movies. In the table below, we show the ratings given by UserIds 13 through 25 for the5 movies with the most ratings. We can see that not evey user has ranked every movie.

```{r, echo=FALSE}
keep <- edx %>% 
  count(movieId) %>% 
  top_n(5, n) %>% 
  .$movieId

tab <- edx %>% 
  filter(movieId%in%keep) %>% 
  filter(userId %in% c(13:25)) %>% 
  select(userId, title, rating) %>% 
  mutate(title = str_remove(title, ", The"),
         title = str_remove(title, ":.*")) %>%
  spread(title, rating)

kable(tab) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

We can get a sense of the sparcity of the data by looking at a matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating. As we can see here, the data is very sparse.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
if(!require(rafalib)) install.packages("rafalib", repos = "http://cran.us.r-project.org")
library(rafalib)

users <- sample(unique(edx$userId), 100)
rafalib::mypar()
edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users") %>% 
  abline(h=0:100+0.5, v=0:100+0.5, col = "lightgrey")

```

Of course, some movies receive a lot more ratings than others. This is not surprising as blockbusters are expected to be rated more frequently than niche movies. We can get a sense of the distribution.

```{r}
edx %>% 
  count(movieId) %>%
  arrange(-n) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  labs(title="Movies by rating count", y="Movies", x="Number of ratings")
```

Of course, the same can be said about users: some are much more active than others.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
edx %>% 
  count(userId) %>%
  arrange(-n) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10()+
  labs(title="Users by rating count", y="Users", x="Number of ratings")
```

### Additional Exploratory Analysis

Let's first identify whether different users tend to rate movies more harshly than others

```{r}
Avg_by_user <- edx %>% 
  group_by(userId) %>% 
  summarize(avg=mean(rating)) 
```

On average, users give an average score of `r mean(Avg_by_user$avg)`. That being said, there is indeed a distribution around this mean.

```{r}  
Avg_by_user %>% ggplot(aes(avg)) +
  geom_histogram(bins = 30, color = "black")+
  labs(title="Distribution of average score by users", x="Average score", y="Number of users")
```


# Training Machine Learning Algorithm

### Creating the Training and Test Set

For this portion of the paper, we will randomly select 20% of the **edx set** to serve as our test set.

```{r}
set.seed(755)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
```

To make sure we don't include users and movies in the test set that do not appear in the training set, we remove these entries using the semi_join function:

```{r}
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```


### Mesure of Success

Typically, we would evaluate our algorithm based on the Loss function, or residual mean squared error (RMSE). We define $y_{u,i}$ as the rating for movie $i$ by user $u$ and denote our prediction with $\hat{y}_{i,u}$. The RMSE is then defined as: 

$$
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
$$
with $N$ being the number of user/movie combinations and the sum occurring over all these combinations.

Let's write a function that computes the **RMSE** for vectors of ratings and their corresponding predictions:

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

Basically, for RMSE greater than 1, it means that on average we are off by more than one star. We therefore try to minimize RMSE.

We are also told that we will be evaluated based on accuracy, not RMSE. This means that we are tasked with making categorical predictions as opposed to numerical predictions. The only values we should be predicting are therefore **0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, or 5**.

The final measure of our success will be calculated as follows:

$$
\mbox{Accuracy}=\frac{Correct\ Predictions}{Total\ Predictions}
$$
Let's write a function that computes the **accuracy** for vectors of ratings and their corresponding predictions:

```{r}
accuracy <- function(true_ratings, predicted_ratings){
  mean(true_ratings==predicted_ratings)
}
```

Let us test the functions:

```{r}
true_ratings <- c(1,2,4.5,4,5,0.5)
predicted_ratings <- c(1,2.5,4.5,4,2,1)

RMSE(true_ratings,predicted_ratings)
accuracy(true_ratings,predicted_ratings)

rm(true_ratings,predicted_ratings)
```

### Our Approach

Given our task, we need to train a machine learning algorithm that will make categorical predictions, not simple continuous prediction. Therefore, we will take a two steps approach.

First, we will create a model that minimizes our RMSE. By drawing on the edX course material, it is likely that the following model will be one that takes into account a "regularized" movie effect and the user-specific effect. Namely, we will create the following model.

$$
Y_{u,i} = \mu + \hat{b}_i(\lambda_i) + \hat{b}_u(\lambda_u) + \varepsilon_{u,i}
$$
In this model, we assume that a given rating (which is categorical in nature) is the sum of $\mu$ the average rating, $\hat{b}_i(\lambda)$ the regularized movie effect (see below for details), $\hat{b}_u(\lambda)$ the regularized user-specific effect and $\varepsilon_{u,i}$ the residual.

Our first task will be to estimate the first three terms and minimize the RMSE. This has the effect of getting us as close to the answer as possible. At this point, however our accuracy will still be close to 0% because we won't have round numbers. 

Then, we will take this list of predictions, and try to estimate the $\varepsilon_{u,i}$ using different machine learning algorithms.

This residual term will help us integrate other effects that influence the rating. It will also bring our predictions to the categorical variable we are targetting.

### The Linear Model

Let us simply check what our success metrics would be if we were to predict the average for each observation.

```{r}
average_rating <- mean(train_set$rating)

naiveRMSE <- RMSE(test_set$rating, average_rating)
naiveAccuracy <- accuracy(test_set$rating, average_rating)
```

As we go along, we will be comparing different approaches. Let's start by creating a results table with this naive approach:

```{r}
rmse_results <- data_frame(Method = "Just the average", RMSE = naiveRMSE, Accuracy =naiveAccuracy)

kable(rmse_results,align=rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Of course, we obtain an accuracy of 0 since the average is not a round number. If we try that we get:

```{r}
rounded_average_rating <- round(average_rating/0.5)*0.5

roundednaiveRMSE <- RMSE(test_set$rating,rounded_average_rating)
roundednaiveAccuracy <- accuracy(test_set$rating,rounded_average_rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "Just the average (rounded)",
                                     RMSE=roundednaiveRMSE,
                                     Accuracy = roundednaiveAccuracy))
kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```
We can see here that we have increased our accuracy to about `r paste(round(100*rmse_results[2,3],2),"%",sep="")`, but our RMSE has deteriorated, which is not surprising.

At this time, we are trying to minimize the residual term, so we will focus on the RMSE, and worry about the accuracy later.

#### Movie-effect

We can now factor in the fact that certain movies tend to get much better ratings than others. We call this the **movie_effect**.

For each movie, the movie effect is calculated as the average of $Y_{u,i} - \hat{\mu}$ for each movie $i$.

We calculate it using this piece of code:

```{r}
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(movie_effect = mean(rating - average_rating))
```

We can see that these estimates vary substantially:

```{r}
movie_avgs %>% qplot(movie_effect, geom ="histogram", bins = 10, data = ., color = I("black"))
```
  
Since our average rating $\mu$ is about 3.5, a 1.5 movie_effect $b_i$ means a perfect score of 5. 
  
We can now try to estimate each observation of the test set using this movie_effect. To be clear, this is the model we are trying, where $\mu$ is the average rating, $b_i$ is the movie_effect and $\varepsilon_{u,i}$ is the error term:
  
$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

  
```{r}
predicted_ratings <- average_rating + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$movie_effect

movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating)
movie_effect_Accuracy <- accuracy(test_set$rating,predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Movie Effect",
                                     RMSE=movie_effect_RMSE,
                                     Accuracy = movie_effect_Accuracy))

kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)

```
  
As we can see, compared to the rounded average, we have improved our RMSE. However, we have only improved by about `r round(rmse_results[1,2]-rmse_results[3,2],3)`.

#### Regularizing the Movie Effect

At this point, we make the hypothesis that some of the large movie effects in our data set are due to a very low number of ratings being available for certain movies. For instance, one movie might have been rated only once and given 5 stars. It would therefore have a very high $b_i$ of ~1.5. There is a case to be made that this shouldn't be the case as we have limited information regarding that movie.

First, let's create a database that connects `movieId` to movie title:

```{r}
movie_titles <- edx %>% 
  select(movieId, title) %>%
  distinct()
```

Here are the 10 best movies according to our estimate and how often they were rated:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(movie_effect)) %>% 
  select(title, movie_effect,n) %>%
  rename("Title"=title,"Movie Effect"=movie_effect,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

And here are the 10 worst:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(movie_effect) %>% 
  select(title, movie_effect,n) %>%
  rename("Title"=title,"Movie Effect"=movie_effect,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Our hypothesis was right: we hare heavily benefiting or penalizing obscure movies when we actually have very little data points for them. These are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.

We will now estimate the **regularized_movie_effect** such that:

$$
Regularized\ Movie\ Effect = \hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$

where $n_i$ is the number of ratings made for movie $i$.

The intuition here is that as $n_i$ gets bigger, the impact of adding $\lambda$ diminishes. However, for small values of $n_i$, the presence of $\lambda$ reduces the estimate of the movie effect $\hat{b}_i(\lambda)$ .

Let's compute these regularized estimates of **regularized_movie_effect** using 
$\lambda=3$. Later, we will obtimize this term.

```{r}
lambda <- 3

movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(reg_movie_effects = sum(rating - average_rating)/(n()+lambda), n_i = n()) 
```

To see how the estimates shrink, let's make a plot of the regularized estimates versus the least squares estimates.

```{r regularization-shrinkage}
data_frame("Original Movie Effect" = movie_avgs$movie_effect, 
           "Regularized Movie Effect" = movie_reg_avgs$reg_movie_effects, 
           n = movie_reg_avgs$n_i) %>%
    ggplot(aes(`Original Movie Effect`, `Regularized Movie Effect`, size=sqrt(n))) + 
        geom_point(shape=1, alpha=0.5)
```

Let's look at the top 10 best movies based on $\hat{b}_i(\lambda)$:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_reg_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(reg_movie_effects)) %>% 
  select(title, reg_movie_effects,n) %>%
  rename("Title"=title,"Reg. Movie Effect"=reg_movie_effects,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

This makes a lot more sense. These movies are widely known as some of the best ever made.

And now the 10 worst:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_reg_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(reg_movie_effects) %>% 
  select(title, reg_movie_effects,n) %>%
  rename("Title"=title,"Reg. Movie Effect"=reg_movie_effects,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Let's see if this improves our results further:

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = average_rating + reg_movie_effects) %>%
  .$pred

reg_movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 
reg_movie_effect_Accuracy <- accuracy(predicted_ratings, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Regularized Movie Effect",
                                     RMSE=reg_movie_effect_RMSE,
                                     Accuracy = reg_movie_effect_Accuracy))

kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)

```

We can see that the improvement from regularization, although quite sensical, is not very large.

This might be due to the fact that we arbitrarily selected $\lambda=3$.

###### Optimizing Lambda for the Movie Effect

Because $\lambda$ is a tuning parameter, we can use cross-validation to choose it. We will do so on the training set only.

```{r}
lambdas <- seq(0, 10, 0.25)

mu <- mean(train_set$rating)
just_the_sum <- train_set %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- test_set %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
```

Therefore, for the best results, we should use this lambda.

```{r}
lambda_movie <- lambdas[which.min(rmses)]
lambda_movie
```

Let's now use this optimized lambda to see how it improves our results.

```{r}
lambda <- lambda_movie

movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(reg_movie_effects = sum(rating - average_rating)/(n()+lambda), n_i = n()) 

predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = average_rating + reg_movie_effects) %>%
  .$pred

reg_movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 
reg_movie_effect_Accuracy <- accuracy(predicted_ratings, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Optimized Regularized Movie Effect",
                                     RMSE=reg_movie_effect_RMSE,
                                     Accuracy = reg_movie_effect_Accuracy))

kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)

```

There is a slight improvement.

#### Adding the User-Specific Effect

Of course, we know that some users also rate movies differently based on their personal characteristics.

```{r}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")+
  labs(title="Count of users by average rating")
```

Here we can calculate the **user_effect** which is a user-specific effect once the **regularized_movie_effect** has been taken into consideration.

To be clear, this is the model we are trying, where $\mu$ is the average rating, $\hat{b}_i(\lambda)$ is the regularized_movie_effect, $b_u$ is the user-specific effect and $\varepsilon_{u,i}$ is the error term:
  
$$
Y_{u,i} = \mu + \hat{b}_i(\lambda) +b_u + \varepsilon_{u,i}
$$

```{r}
user_avgs <- train_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(user_effect = mean(rating - average_rating - reg_movie_effects))
```

Let's create predictions see how it improves our model.

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = average_rating + reg_movie_effects + user_effect) %>%
  .$pred

movieanduser_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 
movieanduser_effect_Accuracy <- accuracy(predicted_ratings, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Optimized Regularized Movie and User Effect",
                                     RMSE=movieanduser_effect_RMSE,
                                     Accuracy = movieanduser_effect_Accuracy))


kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

#### Regularizing the User Effect

For the same reasons we regularized the movie effect, we want to also regularized the user effect. In the code below, we find the best $\lambda_u$ to use for the final model:

$$
Y_{u,i} = \mu + \hat{b}_i(\lambda_i) + \hat{b}_u(\lambda_u) + \varepsilon_{u,i}
$$
```{r best-lambdas}
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){

  mu <- mean(train_set$rating)
  
  reg_b_i <- movie_reg_avgs %>% 
    group_by(movieId) %>%
    summarize(reg_b_i = reg_movie_effects)
  
  reg_b_u <- train_set %>% 
    left_join(reg_b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(reg_b_u = sum(rating - reg_b_i - mu)/(n()+l))

  predicted_ratings <- 
    test_set %>% 
    left_join(reg_b_i, by = "movieId") %>%
    left_join(reg_b_u, by = "userId") %>%
    mutate(pred = mu + reg_b_i + reg_b_u) %>%
    .$pred
  
    return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses)
lambda_user <- lambdas[which.min(rmses)]
```

Therefore, the best $\lambda_u$ to use would be `r lambda_user` and the RMSE we expect would be `r min(rmses)`; very good indeed!

Let's build our final predictions for the linear model.

```{r}
user_reg_avg <- train_set %>% 
    left_join(movie_reg_avgs, by="movieId") %>%
    group_by(userId) %>%
    summarize(reg_user_effects = sum(rating - reg_movie_effects - average_rating)/(n()+lambda_user))

predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  left_join(user_reg_avg, by='userId') %>%
  mutate(pred = average_rating + reg_movie_effects + reg_user_effects) %>%
  .$pred

final_RMSE <- RMSE(predicted_ratings, test_set$rating) 
final_Accuracy <- accuracy(predicted_ratings, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Optimized Regularized Movie and Optimized Regularized User Effect",
                                     RMSE=final_RMSE,
                                     Accuracy = final_Accuracy))


kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

#### The Residuals

Of course, as we can see in the summary table, our accuracy is still 0% because we do not have round numbers. Let's take a look at the distribution of residuals.

```{r}
residuals <- data_frame(Residuals = predicted_ratings-test_set$rating, True_Rating = test_set$rating, Predictions=predicted_ratings )

residuals %>% ggplot(aes(Residuals))+
  geom_histogram(bins = 30, color = "black") + 
  labs(title="Distribution of Residuals", y="Count", x="Residual")
```

We can also look at the relationship between the true ratings and our predictions.

```{r}
fun_mean <- function(x){
  return(data.frame(y=mean(x),2,label=mean(x,na.rm=T)))}

residuals %>% ggplot(aes(x=True_Rating,y=Predictions, group=True_Rating))+
  geom_boxplot()+
  labs(title="Distribution of prediction by True Rating", y="Predictions", x="True Ratings")
```

Here, we see that in the low ratings, we tend to predict too high a rating and in the high ratings we tend to predict too low a rating.

### Using Machine Learning to Estimate $\varepsilon_{u,i}$ in Our Model

Let us remind ourselves where we are. We have the following model, in which $\mu$ is the average rating, $\hat{b}_i(\lambda_i)$ is the optimized regularized movie effect using a $\lambda_i$ of `r lambda_movie`, $\hat{b}_u(\lambda_u)$ is the optimized regularized user effect using a $\lambda_u$ of `r lambda_user`, $\varepsilon_{u,i}$ is the residual term we just analyzed and $Y_{u,i}$ is the true rating.

$$
Y_{u,i} = \mu + \hat{b}_i(\lambda_i) + \hat{b}_u(\lambda_u) + \varepsilon_{u,i}
$$
We currently have a RMSE of `r round(min(rmses),5)`.

We now want to do two things:

1. Ensure that all of our predictions are categorical since we will be evaluted based on accuracy

2. Tease out any other effect that might be burried into the $\varepsilon_{u,i}$

As we go, we will be recording our results in the following table:

```{r}
accuracy_results <- data_frame(Method = "Optimized Linear Model", 
                               RMSE = RMSE(residuals$True_Rating,residuals$Predictions),
                               Accuracy = accuracy(residuals$True_Rating,residuals$Predictions))

kable(accuracy_results,align=rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

If we were to simply round each prediction to the nearest 0.5, we would get:

```{r}
accuracy_results <- bind_rows(accuracy_results,
                          data_frame(Method= "Naive Rounding",
                                     RMSE=RMSE(residuals$True_Rating,round(residuals$Predictions/0.5)*0.5),
                                     Accuracy = accuracy(residuals$True_Rating,round(residuals$Predictions/0.5)*0.5)))

kable(accuracy_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Our accuracy has improved to about `r paste(round(accuracy_results[2,3]*100,2),"%",sep="")`.

Surely, we can do better with a "smarter" approach. 

#### Using a Decision Tree to Create Rounded Predictions

Before moving forward, we free up some memory.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
rm(Avg_by_user,genre_analysis,Genres_Stats,half_vs_full_ratings,just_the_sum,means,movie_avgs,movie_titles,movies_by_number_of_rankings,ratings_freqency,residuals,tab,test_index,user_avgs, current_genre,final_Accuracy,final_RMSE,i,keep,lambda,lambdas,movie_effect_Accuracy,movie_effect_RMSE,movieanduser_effect_Accuracy,movieanduser_effect_RMSE,mu,naiveAccuracy,naiveRMSE,reg_movie_effect_Accuracy,reg_movie_effect_RMSE,rmses,rounded_average_rating,roundednaiveAccuracy,roundednaiveRMSE,users)
```


First, let us create a list of prediction for the training set.

```{r}
predicted_ratings_training <- train_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  left_join(user_reg_avg, by='userId') %>%
  mutate(pred = average_rating + reg_movie_effects + reg_user_effects) %>%
  .$pred

train_set <- train_set %>% 
  mutate(Predictions = predicted_ratings_training)

test_set <- test_set %>% 
  mutate(Predictions = predicted_ratings)


```

In the training set, our RMSE is:

```{r}
RMSE(train_set$rating,train_set$Predictions)
```

If we were to simply round we would get the following accuracy:

```{r}
accuracy(train_set$rating, round(train_set$Predictions/0.5)*0.5)
```

Quite similar to what we had in the test set.

Before training machine learning algorithms, we need to take a sample of the training set, otherwise our computer runs out of RAM.

```{r}
set.seed(755)
sub_index <- createDataPartition(y = train_set$rating, times = 1, p = 0.2, list = FALSE)
sub_train_set <- train_set[sub_index,]
```


Let's now train a decision tree, while transforming the ratings into categorical data so that the decision tree gives us categorical data as well.

```{r}
train_rpart <- train(as.character(rating) ~ Predictions, 
                     method = "rpart",
                     data = sub_train_set)
```

The decision tree looks as follows:

```{r}
plot(train_rpart$finalModel,margin = 0.1)
text(train_rpart$finalModel, cex = 0.75)
```

And our results on the test set would be:

```{r}
tree_predicted_values <- predict(train_rpart,test_set)

tree_RMSE <- RMSE(as.numeric(as.character(tree_predicted_values)),test_set$rating)
tree_accuracy <- accuracy(tree_predicted_values,test_set$rating)

accuracy_results <- bind_rows(accuracy_results,
                          data_frame(Method= "Using Decision Tree with Predicted Values",
                                     RMSE=tree_RMSE,
                                     Accuracy = tree_accuracy))

kable(accuracy_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)

```

We can see here that we have increased our accuracy quite a bit, to `r paste(round(100*accuracy_results[3,3],2),"%",sep="")`.

#### Increasing the Number of Predictors

So far, we've only used the predictions derived from our linear model to train our algorithm. We've completely left out other information such as genre and timestamp. We will now transform our training set to include these variables. 

For the genre, we know there are a total of `r length(genres)` represented. We will therefore add `r length(genres)` columns and indicate with a binary variable whether the movie had that genre listed.

We first create a rubric of all movie IDs and whether they associate to each genre.

```{r}
rm(sub_train_set)

moviegenres <- edx %>% 
  select(movieId,genres) %>% 
  distinct()

moviegenres[genres] <- ""

for (i in 1:length(genres)) {
  moviegenres[,2+i] <- ifelse(str_detect(moviegenres$genres,genres[i]),1,0)
}

moviegenres <- moviegenres[-2]

kable(head(moviegenres[,1:6])) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)

```

We then add this information back into our training and testing sets.

```{r}
train_set <- train_set %>% 
  left_join(moviegenres, by="movieId")

test_set <- test_set %>% 
  left_join(moviegenres, by="movieId")

edx <- edx %>% 
  left_join(moviegenres, by="movieId")

validation <- validation %>% 
  left_join(moviegenres, by="movieId")

```

Let us now re-create our sub-training-set.

```{r}
set.seed(755)
sub_index <- createDataPartition(y = train_set$rating, times = 1, p = 0.01, list = FALSE)
sub_train_set <- train_set[sub_index,]

genreslist <- genres
rm(genres)

sub_train_set <- sub_train_set %>% 
  select(rating,Predictions,timestamp,one_of(genreslist))

sub_train_set$rating <- as.character(sub_train_set$rating)
```

#### Trying Various Algorithms to Maximize Accuracy

Here, we will train a vast number of algorithms on the sub_training_set. We will then create a matrix of prediction on the full training set. 

```{r}
models <- c( "lda", "naive_bayes","knn", "kknn", "Rborist")
```

```{r}
fits <- lapply(models, function(model){ 
	print(model)
	train(rating ~ ., 
	      method = model, 
	      data = sub_train_set)
}) 
    
names(fits) <- models
```

